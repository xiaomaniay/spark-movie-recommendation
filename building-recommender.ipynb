{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define download locations\n",
    "import os\n",
    "\n",
    "datasets_path = os.path.join('datasets')\n",
    "\n",
    "complete_dataset_path = os.path.join(datasets_path, 'ml-latest.zip')\n",
    "small_dataset_path = os.path.join(datasets_path, 'ml-latest-small.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proceed with downloads\n",
    "import urllib.request\n",
    "\n",
    "small_f = urllib.request.urlretrieve(small_dataset_url, small_dataset_path)\n",
    "complete_f = urllib.request.urlretrieve(complete_dataset_url, complete_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract rating data into their invididual folders\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(small_dataset_path, 'r') as z:\n",
    "    z.extractall(datasets_path)\n",
    "with zipfile.ZipFile(complete_dataset_path, 'r') as z:\n",
    "    z.extractall(datasets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod the raw ratings data and filter out the header\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "small_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
    "\n",
    "small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
    ".map(lambda line: line.split(',')).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
    "\n",
    "small_movies_raw_data = sc.textFile(small_movies_file)\n",
    "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
    ".map(lambda line: line.split(',')).map(lambda tokens: (tokens[0], tokens[1])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movieId,title,genres'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_movies_raw_data_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '1', '4.0'),\n",
       " ('1', '3', '4.0'),\n",
       " ('1', '6', '4.0'),\n",
       " ('1', '47', '5.0'),\n",
       " ('1', '50', '5.0')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ratings_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the small dataset into train, validation and test datasets\n",
    "training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.9135506292157884\n",
      "For rank 8 the RMSE is 0.9133253198173535\n",
      "For rank 12 the RMSE is 0.9194236820985827\n",
      "The best model was trained with rank 8\n"
     ]
    }
   ],
   "source": [
    "# model training with small dataset\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "seed = 5\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n",
    "                     lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1]) ** 2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print('For rank %s the RMSE is %s' % (rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "print('The best model was trained with rank %s' % best_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 1084), 3.6227087809721867),\n",
       " ((156, 1084), 3.8102899221717266),\n",
       " ((372, 1084), 3.5483496000160977)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 47), (5.0, 4.779416113993275)),\n",
       " ((1, 457), (5.0, 4.505201388257333)),\n",
       " ((1, 527), (5.0, 5.17322824176339))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_and_preds.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.9134290435791891\n"
     ]
    }
   ],
   "source": [
    "# test the selected model\n",
    "model = ALS. train(training_RDD, best_rank, seed=seed, iterations=iterations, \n",
    "                  lambda_=regularization_parameter)\n",
    "predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1]) ** 2).mean())\n",
    "print('For testing data the RMSE is %s' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27753444 recommendations in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "# using the complete dataset to build the final model\n",
    "# load the complete datast file\n",
    "complete_ratings_file = os.path.join(datasets_path, 'ml-latest', 'ratings.csv')\n",
    "complete_ratings_raw_data = sc.textFile(complete_ratings_file)\n",
    "complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n",
    "\n",
    "# parse the data\n",
    "complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
    "    \n",
    "print('There are %s recommendations in the complete dataset' % (complete_ratings_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the recommender model\n",
    "training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0)\n",
    "\n",
    "complete_model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations, lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.8213568307148696\n"
     ]
    }
   ],
   "source": [
    "# test the complete model on testing set\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1]) ** 2).mean())\n",
    "\n",
    "print('For testing data the RMSE is %s' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the movies complete file for later use\n",
    "complete_movies_file = os.path.join(datasets_path, 'ml-latest', 'movies.csv')\n",
    "complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
    "complete_movies_data_header = complete_movies_raw_data.take(1)[0]\n",
    "\n",
    "# parse the data\n",
    "complete_movies_data = complete_movies_raw_data.filter(lambda line: line != complete_movies_data_header)\\\n",
    ".map(lambda line: line.split(',')).map(lambda tokens: (int(tokens[0]), tokens[1], tokens[2])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58098 movies in the complete datasets\n"
     ]
    }
   ],
   "source": [
    "complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]), x[1]))\n",
    "print('There are %s movies in the complete datasets' % (complete_movies_titles.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'complete_ratings_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d83520eddb94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mID_and_ratings_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnratings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mID_and_ratings_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnratings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmovie_ID_with_ratings_RDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomplete_ratings_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmovie_ID_with_avg_ratings_RDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmovie_ID_with_ratings_RDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_counts_and_averages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmovie_rating_counts_RDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmovie_ID_with_avg_ratings_RDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complete_ratings_data' is not defined"
     ]
    }
   ],
   "source": [
    "# calculate average ratings and total number of ratings for each movie\n",
    "def get_counts_and_averages(ID_and_ratings_tuple):\n",
    "    nratings = len(ID_and_ratings_tuple[1])\n",
    "    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1])) / nratings)\n",
    "\n",
    "movie_ID_with_ratings_RDD = complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey()\n",
    "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 260, 9), (0, 1, 8), (0, 16, 7), (0, 25, 8), (0, 32, 9), (0, 335, 4), (0, 379, 3), (0, 296, 7), (0, 858, 10), (0, 50, 8)]\n"
     ]
    }
   ],
   "source": [
    "# add new user ratings and put them in a new RDD\n",
    "new_user_ID = 0\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "new_user_ratings = [\n",
    "     (0,260,9), # Star Wars (1977)\n",
    "     (0,1,8), # Toy Story (1995)\n",
    "     (0,16,7), # Casino (1995)\n",
    "     (0,25,8), # Leaving Las Vegas (1995)\n",
    "     (0,32,9), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
    "     (0,335,4), # Flintstones, The (1994)\n",
    "     (0,379,3), # Timecop (1994)\n",
    "     (0,296,7), # Pulp Fiction (1994)\n",
    "     (0,858,10) , # Godfather, The (1972)\n",
    "     (0,50,8) # Usual Suspects, The (1995)\n",
    "    ]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print('New user ratings: %s' % new_user_ratings_RDD.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the new user ratings to the training data for recommender model\n",
    "complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model trained in 156.641 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the ALS model using all the parameters seleted before\n",
    "from time import time\n",
    "t0 = time()\n",
    "new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed,\n",
    "                             iterations=iterations, lambda_=regularization_parameter)\n",
    "tt = time() - t0\n",
    "\n",
    "print('New model trained in %s seconds' % round(tt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_movies_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting top recommendations\n",
    "# get an RDD with all the movies that the new user hasn't rated yet\n",
    "new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs\n",
    "# keep just those not on the ID list\n",
    "new_user_unrated_movies_RDD = complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (0, 3)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_unrated_movies_RDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the input RDD, new_users_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=116688, rating=1.942818887104374),\n",
       " Rating(user=0, product=57044, rating=5.872604571679518)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_recommendations_RDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(125970, ((4.0670946283444085, 'Halloweentown (1998)'), 148)),\n",
       " (7410, ((5.169659204023402, '\"Osterman Weekend'), 177)),\n",
       " (163020, ((5.2543450405208345, 'The Fits (2016)'), 37))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Toy Story (1995)')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_movies_titles.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1449, 6867)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_rating_counts_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Halloweentown (1998)', 4.0670946283444085, 148)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommended movies (with more than 25 reviews):\n",
      " ('Cosmos', 8.77380602282307, 157)\n",
      "('\"Godfather', 8.673892677504824, 60904)\n",
      "('Music for One Apartment and Six Drummers (2001)', 8.660283657976892, 31)\n",
      "('Planet Earth II (2016)', 8.572289265784786, 853)\n",
      "('Planet Earth (2006)', 8.548089885641577, 1384)\n",
      "('Band of Brothers (2001)', 8.525198248300295, 984)\n",
      "('Frozen Planet (2011)', 8.512755610097, 402)\n",
      "('\"Godfather: Part II', 8.497728955132377, 38875)\n",
      "('Seven Samurai (Shichinin no samurai) (1954)', 8.420076488054455, 14578)\n",
      "('Star Wars: Episode V - The Empire Strikes Back (1980)', 8.379513705645964, 65822)\n",
      "('\"Civil War', 8.370763863325083, 431)\n",
      "('The Godfather Trilogy: 1972-1990 (1992)', 8.364873099117656, 421)\n",
      "('Star Wars: Episode IV - A New Hope (1977)', 8.362069511992544, 81815)\n",
      "('\"Lord of the Rings: The Fellowship of the Ring', 8.358713928000869, 61883)\n",
      "('Casablanca (1942)', 8.353733164606588, 31095)\n",
      "('Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', 8.351228931229326, 29484)\n",
      "('Life (2009)', 8.35027474829656, 166)\n",
      "('Magic & Bird: A Courtship of Rivals (2010)', 8.345311085455442, 27)\n",
      "('\"Shawshank Redemption', 8.341142630217224, 97999)\n",
      "('\"Woman In Berlin', 8.340527738419574, 39)\n",
      "(\"Glen Campbell: I'll Be Me (2014)\", 8.336007529604146, 30)\n",
      "('\"Usual Suspects', 8.335364308249186, 62180)\n",
      "('Slaying the Badger', 8.332531251896054, 25)\n",
      "(\"Long Night's Journey Into Day (2000)\", 8.30467138505386, 35)\n",
      "('\"Lord of the Rings: The Return of the King', 8.29996418825304, 57378)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2] >= 25).takeOrdered(25, key=lambda x: -x[1])\n",
    "print('Top recommended movies (with more than 25 reviews):\\n %s' % '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=500, rating=5.266544001099173)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
    "individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
    "individual_movie_rating_RDD.filter(lambda x: x[1] == 500).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=500, rating=5.266544001099173)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
    "individual_movie_rating_RDD = new_ratings_model.predictAll(my_movie)\n",
    "individual_movie_rating_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o352.save.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/C:/Users/Xiaoman/Desktop/spark-movie-recommendation/models/movie_lens_als/metadata already exists\r\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\r\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel$SaveLoadV1_0$.save(MatrixFactorizationModel.scala:367)\r\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel.save(MatrixFactorizationModel.scala:205)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-ac3838158c6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Save and load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0msame_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatrixFactorizationModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\mllib\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sc, path)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o352.save.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/C:/Users/Xiaoman/Desktop/spark-movie-recommendation/models/movie_lens_als/metadata already exists\r\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\r\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel$SaveLoadV1_0$.save(MatrixFactorizationModel.scala:367)\r\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel.save(MatrixFactorizationModel.scala:205)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n"
     ]
    }
   ],
   "source": [
    "# persisting the model\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "\n",
    "model_path = os.path.join('models', 'movie_lens_als')\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, model_path)\n",
    "same_model = MatrixFactorizationModel.load(sc, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
